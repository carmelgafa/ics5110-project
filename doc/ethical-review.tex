\section{Ethical Review}

	This subject has been already discussed into some details in Section \ref{secn:data-exploration} where we have shown how the COMPAS model affects the different race groups and genders. In this study we have also investigated the robustness of the COMPAS decile score when compared to actual two year recidivism rate. In Section \ref{secn:experiments} we had the opportunity to see how fifferen loss functions and model parameters affect the metrics that are obtained. The dataset examined in this study is particularly significant, as its findings have real-world implications that directly impact individuals and their families. False negatives and false positives mean that potentially dangerous individuals are put on the streets and others might receive an unjustly harsh treatment.

	The analysis of the data, model performance, and insights generated raises several ethical concerns and societal implications that warrant careful evaluation. First, the dataset used for training likely reflects inherent biases from historical or systemic inequalities, particularly in domains such as recidivism prediction. If the dataset is biased, models trained on it, regardless of their performance metrics, may inadvertently perpetuate or amplify these biases. For instance, minority groups may face disproportionately higher false positive or false negative rates, which could lead to unjust outcomes such as harsher treatment or denied opportunities.
	
	The insights generated by these models also need to be critically examined. While models like \texttt{knn distance} demonstrated near-perfect performance on the test set, such high metrics often indicate overfitting, meaning the model may not generalize well to new or unseen data. Overfitting, combined with biased data, could result in overly confident predictions that fail to account for the complexities of real-world cases. This raises questions about the trustworthiness of these insights, especially in high-stakes decisions that affect individuals' lives.
	
	From a societal perspective, the implications of deploying these models are profound. A model with high false positive rates for the minority class (e.g., labeling individuals as high risk when they are not) can lead to unfair treatment, reinforcing discrimination and systemic inequities. Conversely, high false negative rates (e.g., failing to identify high-risk individuals) could pose public safety concerns. These outcomes highlight the importance of balancing precision and recall while ensuring fairness and transparency in predictions.
	
	To mitigate these ethical risks, several measures are necessary. Regular bias audits should be conducted on the dataset and model outputs to identify and address disparities. Additionally, stakeholders must be involved in the development process to ensure that the models align with societal values and legal standards. Explainability techniques should also be employed to make model predictions more interpretable, allowing decision-makers to understand the rationale behind predictions. Finally, models should be rigorously tested on diverse and representative datasets to ensure robustness and fairness across different demographic groups.
	
	In summary, while the results demonstrate strong technical performance, the ethical considerations of bias, fairness, and societal impact must remain at the forefront. Developing and deploying machine learning models in sensitive domains requires a careful balance between accuracy and equity, with continuous efforts to ensure accountability and mitigate harm.
	




