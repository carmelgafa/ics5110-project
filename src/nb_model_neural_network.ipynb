{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = \"../results\"\n",
    "DATA_FOLDER = \"../data\"\n",
    "TEMP_FOLDER = \"../tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training dataset\n",
    "\n",
    "Load the train dataset in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_path = os.path.join(DATA_FOLDER, 'train_dataset.csv')\n",
    "df_train = pd.read_csv(df_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create additional features\n",
    "\n",
    "We create the following additional features:\n",
    "\n",
    "- `history_of_violence` - sum of all violence-related crimes in the past\n",
    "- `socioeconomic_stability` - 1 / (1 + `priors_count`). If no priors count this will be equal to 1 (good stability), otherwise it will start getting smaller with each increase of priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"history_of_violence\"] = (\n",
    "    df_train[\"juv_fel_count\"] +\n",
    "    df_train[\"juv_misd_count\"] +\n",
    "    df_train[\"juv_other_count\"] +\n",
    "    df_train[\"priors_count\"]\n",
    ")\n",
    "\n",
    "# Socioeconomic stability proxy\n",
    "df_train[\"socioeconomic_stability\"] = (1 / (1 + df_train[\"priors_count\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model training\n",
    "\n",
    "- Select features to be used for training\n",
    "    - `age`\n",
    "    - `priors_count`\n",
    "    - `history_of_violence`\n",
    "    - `days_b_screening_arrest`\n",
    "    - `socioeconomic_stability`\n",
    "    - `c_charge_degree_F`\n",
    "    - `c_charge_degree_M`\n",
    "- Scale all features, mean 0 and std dev 1\n",
    "\n",
    "\n",
    "- Select the label for training\n",
    "    - `two_year_recid` * 10 to put the scale between 0 and 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = df_train[[ \n",
    "    \"age\", \"priors_count\", \"history_of_violence\", \n",
    "    \"socioeconomic_stability\", \"c_charge_degree_F\", \"c_charge_degree_M\"\n",
    "]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "y_train = df_train[\"two_year_recid\"] * 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5771, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5771 entries, 0 to 5770\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       5771 non-null   float64\n",
      " 1   1       5771 non-null   float64\n",
      " 2   2       5771 non-null   float64\n",
      " 3   3       5771 non-null   float64\n",
      " 4   4       5771 non-null   float64\n",
      " 5   5       5771 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 270.6 KB\n",
      "None\n",
      "                  0             1             2             3             4  \\\n",
      "count  5.771000e+03  5.771000e+03  5.771000e+03  5.771000e+03  5.771000e+03   \n",
      "mean  -1.274323e-16  3.078075e-17 -4.432427e-17 -1.280479e-16 -1.083482e-16   \n",
      "std    1.000087e+00  1.000087e+00  1.000087e+00  1.000087e+00  1.000087e+00   \n",
      "min   -1.408046e+00 -7.109982e-01 -7.163197e-01 -1.307391e+00 -1.365291e+00   \n",
      "25%   -8.143374e-01 -7.109982e-01 -7.163197e-01 -9.133677e-01 -1.365291e+00   \n",
      "50%   -3.054445e-01 -2.990746e-01 -3.310558e-01 -4.477038e-01  7.324445e-01   \n",
      "75%    6.275258e-01  3.188107e-01  2.468399e-01  1.414952e+00  7.324445e-01   \n",
      "max    5.207562e+00  7.115549e+00  7.759484e+00  1.414952e+00  7.324445e-01   \n",
      "\n",
      "                  5  \n",
      "count  5.771000e+03  \n",
      "mean  -1.969968e-17  \n",
      "std    1.000087e+00  \n",
      "min   -7.324445e-01  \n",
      "25%   -7.324445e-01  \n",
      "50%   -7.324445e-01  \n",
      "75%    1.365291e+00  \n",
      "max    1.365291e+00  \n",
      "          0         1         2         3         4         5\n",
      "0  0.457895  0.112849  0.054208 -0.820235  0.732445 -0.732445\n",
      "1  3.426437  0.318811  0.246840 -0.913368  0.732445 -0.732445\n",
      "2 -0.814337 -0.505036 -0.523688  0.017960  0.732445 -0.732445\n",
      "3  1.051603 -0.710998 -0.716320  1.414952 -1.365291  1.365291\n",
      "4 -1.153599 -0.710998 -0.716320  1.414952  0.732445 -0.732445\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(pd.DataFrame(X_train_scaled).info())\n",
    "print(pd.DataFrame(X_train_scaled).describe())\n",
    "print(pd.DataFrame(X_train_scaled).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a custom dataset\n",
    "class COMPASDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Define the Neural Network\n",
    "class RiskScoreNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RiskScoreNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 10)  # 10 neurons for 10 risk scores\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# df = pd.read_csv(\"compas-dataset.csv\")  # Replace with your dataset file\n",
    "# features = df.drop(columns=[\"two_year_recid\", \"decile_score\"])  # Drop label and COMPAS score\n",
    "# labels = df[\"two_year_recid\"]  # Use the two_year_recid label\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = COMPASDataset(X_train_scaled, y_train)\n",
    "# test_dataset = COMPASDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train_scaled.shape[1]\n",
    "model = RiskScoreNN(input_size)\n",
    "criterion = nn.CrossEntropyLoss()  # For multiclass classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# compas_correct = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in test_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == targets).sum().item()\n",
    "#         total += targets.size(0)\n",
    "\n",
    "#         # Compare with COMPAS decile_score (optional)\n",
    "#         compas_scores = df.loc[test_dataset.indices, \"decile_score\"]  # Assuming decile_score exists\n",
    "#         compas_pred = (compas_scores > 5).astype(int)  # Example threshold\n",
    "#         compas_correct += (compas_pred == targets.numpy()).sum()\n",
    "\n",
    "# print(f\"Model Accuracy: {correct / total * 100:.2f}%\")\n",
    "# print(f\"COMPAS Accuracy: {compas_correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N_INPUT = 6\n",
    "# number of neurons in the hidden layer of the MLP\n",
    "N_HIDDEN = 64\n",
    "\n",
    "# define network parameters\n",
    "GEN = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "# input layer\n",
    "C = torch.randn((vocab_size, N_INPUT),\n",
    "                 generator=GEN, device=device)\n",
    "\n",
    "# hidden layer\n",
    "w1 = torch.randn((N_INPUT*N_BLOCK, N_HIDDEN),\n",
    "                 generator=GEN, device=device) * (5/3) / ((N_BLOCK * N_INPUT)**0.5)\n",
    "\n",
    "# useless because of batchnorm\n",
    "b1 = torch.randn((N_HIDDEN,1),\n",
    "                 generator=GEN, device=device) * 0.1\n",
    "\n",
    "# output layer\n",
    "w2 = torch.randn((N_HIDDEN, vocab_size),\n",
    "                 generator=GEN, device=device) * 0.1\n",
    "b2 = torch.randn( (vocab_size, 1),\n",
    "                 generator=GEN, device=device) * 0.1\n",
    "\n",
    "# batch normalization\n",
    "bn_gain = torch.randn((1, N_HIDDEN), generator=GEN, device=device) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, N_HIDDEN), generator=GEN, device=device) * 0.1\n",
    "\n",
    "# network params\n",
    "parameters = [C, w1, b1, w2, b2, bn_gain, bn_bias]\n",
    "# print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# network definition\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "N_BATCH = torch.randint(0, X_train.shape[0], (BATCH_SIZE, ), generator=GEN, device=device)\n",
    "x_batch, y_batch = X_train[N_BATCH], Y_train[N_BATCH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras' has no attribute 'wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 51\u001b[0m\n\u001b[0;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     46\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Combine preprocessing and training in a pipeline\u001b[39;00m\n\u001b[0;32m     50\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 51\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrappers\u001b[49m\u001b[38;5;241m.\u001b[39mscikit_learn\u001b[38;5;241m.\u001b[39mKerasClassifier(\n\u001b[0;32m     52\u001b[0m         build_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: model,\n\u001b[0;32m     53\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     54\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     55\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m     ))\n\u001b[0;32m     57\u001b[0m ])\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras' has no attribute 'wrappers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Load your dataset\n",
    "# # Assuming you have a CSV file, replace 'dataset.csv' with your file path\n",
    "# df_train = pd.read_csv('dataset.csv')\n",
    "\n",
    "# # Preprocess features and target\n",
    "# # Assuming 'two_year_recid' is your target column and 'decile_score' is used for comparison\n",
    "# target_column = 'two_year_recid'\n",
    "# features = df_train.drop(columns=[target_column, 'decile_score'])  # Drop unused columns\n",
    "# target = df_train[target_column]\n",
    "\n",
    "# # Create decile bins if necessary (e.g., scale target into 0-9)\n",
    "# # Assuming target is binary, you can scale it into deciles based on conditions or continuous scores\n",
    "# # For simplicity, assume it is already in 0-9 deciles.\n",
    "\n",
    "# # Train-Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "# categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numeric_features),\n",
    "#         ('cat', OneHotEncoder(), categorical_features)\n",
    "#     ])\n",
    "\n",
    "# Build the TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # 10 neurons for deciles\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Combine preprocessing and training in a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('model', tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "        build_fn=lambda: model,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(preprocessor.transform(X_test), y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict and compare with COMPAS\n",
    "y_pred = model.predict(preprocessor.transform(X_test))\n",
    "print(f\"Predicted deciles: {np.argmax(y_pred, axis=1)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
