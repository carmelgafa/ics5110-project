{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = \"../results\"\n",
    "DATA_FOLDER = \"../data\"\n",
    "TEMP_FOLDER = \"../tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training dataset\n",
    "\n",
    "Load the train dataset in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_path = os.path.join(DATA_FOLDER, 'train_dataset.csv')\n",
    "df_train = pd.read_csv(df_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create additional features\n",
    "\n",
    "We create the following additional features:\n",
    "\n",
    "- `history_of_violence` - sum of all violence-related crimes in the past\n",
    "- `socioeconomic_stability` - 1 / (1 + `priors_count`). If no priors count this will be equal to 1 (good stability), otherwise it will start getting smaller with each increase of priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"history_of_violence\"] = (\n",
    "    df_train[\"juv_fel_count\"] +\n",
    "    df_train[\"juv_misd_count\"] +\n",
    "    df_train[\"juv_other_count\"] +\n",
    "    df_train[\"priors_count\"]\n",
    ")\n",
    "\n",
    "# Socioeconomic stability proxy\n",
    "df_train[\"socioeconomic_stability\"] = (1 / (1 + df_train[\"priors_count\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model training\n",
    "\n",
    "- Select features to be used for training\n",
    "    - `age`\n",
    "    - `priors_count`\n",
    "    - `history_of_violence`\n",
    "    - `days_b_screening_arrest`\n",
    "    - `socioeconomic_stability`\n",
    "    - `c_charge_degree_F`\n",
    "    - `c_charge_degree_M`\n",
    "- Scale all features, mean 0 and std dev 1\n",
    "\n",
    "\n",
    "- Select the label for training\n",
    "    - `two_year_recid` * 10 to put the scale between 0 and 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = df_train[[ \n",
    "    \"age\", \"priors_count\", \"history_of_violence\", \n",
    "    \"socioeconomic_stability\", \"c_charge_degree_F\", \"c_charge_degree_M\"\n",
    "]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "y_train = df_train[\"two_year_recid\"] * 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(pd.DataFrame(X_train_scaled).info())\n",
    "print(pd.DataFrame(X_train_scaled).describe())\n",
    "print(pd.DataFrame(X_train_scaled).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df_train_path = os.path.join(DATA_FOLDER, 'train_dataset.csv')\n",
    "df_train = pd.read_csv(df_train_path)\n",
    "\n",
    "# Feature engineering\n",
    "df_train[\"history_of_violence\"] = (\n",
    "    df_train[\"juv_fel_count\"] +\n",
    "    df_train[\"juv_misd_count\"] +\n",
    "    df_train[\"juv_other_count\"] +\n",
    "    df_train[\"priors_count\"]\n",
    ")\n",
    "\n",
    "# Socioeconomic stability proxy\n",
    "df_train[\"socioeconomic_stability\"] = (1 / (1 + df_train[\"priors_count\"]))\n",
    "\n",
    "# Select features and target\n",
    "X_train = df_train[[ \n",
    "    \"age\", \"priors_count\", \"history_of_violence\", \n",
    "    \"socioeconomic_stability\", \"c_charge_degree_F\", \"c_charge_degree_M\"\n",
    "]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "y_train = (df_train[\"two_year_recid\"] * 10)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_dev_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_dev_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Define the neural network model\n",
    "class RiskScoreModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RiskScoreModel, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # BatchNorm after the first hidden layer\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)  # BatchNorm after the second hidden layer\n",
    "        self.output = nn.Linear(32, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.hidden1(x)))  # Apply BN and ReLU\n",
    "        x = self.relu(self.bn2(self.hidden2(x)))  # Apply BN and ReLU\n",
    "        x = self.softmax(self.output(x))  # Apply Softmax\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "N_INPUT = X_train.shape[1]\n",
    "N_OUTPUT = 11  # 10 classes\n",
    "model = RiskScoreModel(N_INPUT, N_OUTPUT)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "num_batches = len(X_train_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_train_tensor[start:end]\n",
    "        y_batch = y_train_tensor[start:end]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_accuracies = []  \n",
    "    \n",
    "    # # Validation step\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     val_outputs = model(X_val_tensor)\n",
    "    #     val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    #     val_preds = torch.argmax(val_outputs, dim=1)\n",
    "    #     val_accuracy = (val_preds == y_val_tensor).float().mean()\n",
    "    #     val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy.item():.4f}\")\n",
    "\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_train_tensor)\n",
    "        val_loss = criterion(val_outputs, y_train_tensor)\n",
    "        val_preds = torch.argmax(val_outputs, dim=1)\n",
    "        val_accuracy = (val_preds == y_train_tensor ).float().mean()\n",
    "        val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_dev_tensor)\n",
    "    val_preds = torch.argmax(val_outputs, dim=1)\n",
    "    print(classification_report(y_dev_tensor, val_preds))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5771, 27)\n",
      "(722, 27)\n",
      "(5771, 6) (5771,)\n",
      "(722, 6) (722,)\n",
      "number of parameters: 3050\n",
      "Epoch 1/50, Loss: 2.0245, Val Loss: 1.8206\n",
      "Epoch 2/50, Loss: 1.9582, Val Loss: 1.7868\n",
      "Epoch 3/50, Loss: 1.7638, Val Loss: 1.7891\n",
      "Epoch 4/50, Loss: 1.7892, Val Loss: 1.7908\n",
      "Epoch 5/50, Loss: 2.1527, Val Loss: 1.7892\n",
      "Epoch 6/50, Loss: 1.7947, Val Loss: 1.7901\n",
      "Epoch 7/50, Loss: 1.7311, Val Loss: 1.7891\n",
      "Epoch 8/50, Loss: 1.9237, Val Loss: 1.7903\n",
      "Epoch 9/50, Loss: 1.7339, Val Loss: 1.7933\n",
      "Epoch 10/50, Loss: 1.6219, Val Loss: 1.7932\n",
      "Epoch 11/50, Loss: 1.7866, Val Loss: 1.7874\n",
      "Epoch 12/50, Loss: 1.7372, Val Loss: 1.7913\n",
      "Epoch 13/50, Loss: 1.8281, Val Loss: 1.7883\n",
      "Epoch 14/50, Loss: 1.5565, Val Loss: 1.7837\n",
      "Epoch 15/50, Loss: 1.7112, Val Loss: 1.7922\n",
      "Epoch 16/50, Loss: 1.6870, Val Loss: 1.7861\n",
      "Epoch 17/50, Loss: 1.5614, Val Loss: 1.7889\n",
      "Epoch 18/50, Loss: 1.6341, Val Loss: 1.7881\n",
      "Epoch 19/50, Loss: 1.6426, Val Loss: 1.7851\n",
      "Epoch 20/50, Loss: 1.6223, Val Loss: 1.7871\n",
      "Epoch 21/50, Loss: 1.5869, Val Loss: 1.7832\n",
      "Epoch 22/50, Loss: 2.1054, Val Loss: 1.7870\n",
      "Epoch 23/50, Loss: 1.8093, Val Loss: 1.7826\n",
      "Epoch 24/50, Loss: 1.7272, Val Loss: 1.7896\n",
      "Epoch 25/50, Loss: 1.7411, Val Loss: 1.7792\n",
      "Epoch 26/50, Loss: 1.7893, Val Loss: 1.7805\n",
      "Epoch 27/50, Loss: 1.7482, Val Loss: 1.7876\n",
      "Epoch 28/50, Loss: 1.6962, Val Loss: 1.7844\n",
      "Epoch 29/50, Loss: 1.6187, Val Loss: 1.7869\n",
      "Epoch 30/50, Loss: 1.7507, Val Loss: 1.7823\n",
      "Epoch 31/50, Loss: 1.8287, Val Loss: 1.7835\n",
      "Epoch 32/50, Loss: 1.5566, Val Loss: 1.7849\n",
      "Epoch 33/50, Loss: 1.6957, Val Loss: 1.7844\n",
      "Epoch 34/50, Loss: 1.6770, Val Loss: 1.7794\n",
      "Epoch 35/50, Loss: 1.9920, Val Loss: 1.7847\n",
      "Epoch 36/50, Loss: 1.6255, Val Loss: 1.7823\n",
      "Epoch 37/50, Loss: 1.7243, Val Loss: 1.7833\n",
      "Epoch 38/50, Loss: 1.7344, Val Loss: 1.7853\n",
      "Epoch 39/50, Loss: 1.6771, Val Loss: 1.7821\n",
      "Epoch 40/50, Loss: 1.5564, Val Loss: 1.7864\n",
      "Epoch 41/50, Loss: 1.8546, Val Loss: 1.7794\n",
      "Epoch 42/50, Loss: 1.6426, Val Loss: 1.7901\n",
      "Epoch 43/50, Loss: 1.8734, Val Loss: 1.7838\n",
      "Epoch 44/50, Loss: 1.7910, Val Loss: 1.7853\n",
      "Epoch 45/50, Loss: 1.7730, Val Loss: 1.7846\n",
      "Epoch 46/50, Loss: 1.9082, Val Loss: 1.7813\n",
      "Epoch 47/50, Loss: 2.0719, Val Loss: 1.7844\n",
      "Epoch 48/50, Loss: 1.6424, Val Loss: 1.7794\n",
      "Epoch 49/50, Loss: 1.8405, Val Loss: 1.7896\n",
      "Epoch 50/50, Loss: 1.8326, Val Loss: 1.7877\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df_train_path = os.path.join(DATA_FOLDER, 'train_dataset.csv')\n",
    "df_train = pd.read_csv(df_train_path)\n",
    "\n",
    "df_dev_path = os.path.join(DATA_FOLDER, 'dev_dataset.csv')\n",
    "df_dev = pd.read_csv(df_dev_path)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_dev.shape)\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"history_of_violence\"] = (\n",
    "        df[\"juv_fel_count\"] +\n",
    "        df[\"juv_misd_count\"] +\n",
    "        df[\"juv_other_count\"] +\n",
    "        df[\"priors_count\"]\n",
    "    )\n",
    "\n",
    "    # Socioeconomic stability proxy\n",
    "    df[\"socioeconomic_stability\"] = (1 / (1 + df[\"priors_count\"]))\n",
    "\n",
    "    X = df[[ \"age\", \"priors_count\", \"history_of_violence\",\n",
    "                \"socioeconomic_stability\", \"c_charge_degree_F\", \"c_charge_degree_M\"]]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X_train_scaled = feature_engineering(df_train)\n",
    "X_dev_scaled = feature_engineering(df_dev)\n",
    "\n",
    "# prepare targets for train and validation\n",
    "y_train = (df_train[\"two_year_recid\"] * 10).clip(0,9)\n",
    "y_dev = (df_dev[\"two_year_recid\"] * 10).clip(0,9)\n",
    "\n",
    "print(X_train_scaled.shape, y_train.shape)\n",
    "print(X_dev_scaled.shape, y_dev.shape)\n",
    "\n",
    "\n",
    "# convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_dev_tensor = torch.tensor(X_dev_scaled, dtype=torch.float32)\n",
    "y_dev_tensor = torch.tensor(y_dev.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "N_INPUT = X_train_tensor.shape[1]\n",
    "N_OUTPUT = 10\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(N_INPUT, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, N_OUTPUT),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(\"number of parameters:\", sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "number_of_samples = X_train_tensor.size(0)  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices = torch.randperm(number_of_samples)\n",
    "    X_train_shuffled = X_train_tensor[indices]\n",
    "    y_train_shuffled = y_train_tensor[indices]\n",
    "    \n",
    "    # Loop through mini-batches\n",
    "    for i in range(0, number_of_samples, batch_size):\n",
    "        # Get the mini-batch\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_dev_tensor)\n",
    "        val_loss = criterion(val_outputs, y_dev_tensor)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "        # for layer in layers:\n",
    "        #     layer.out.retain_grad()\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # epoch_train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    # Validation step\n",
    "    # x = X_dev_tensor\n",
    "    # for layer in layers:\n",
    "    #     x = layer(x)\n",
    "    # loss = F.cross_entropy(x, y_dev_tensor)\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 10,  1, 10,  1, 10,  1, 10, 10,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "risk_metrics = torch.argmax(outputs, dim=1)+1\n",
    "\n",
    "print(risk_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_two_year_recid(pred_type, y_pred):\n",
    "    '''\n",
    "    categorize the predicted scores into low, medium, and high risk groups\n",
    "    '''\n",
    "    \n",
    "    def categorize_score(score):\n",
    "        if score <= 4:\n",
    "            return \"Low\"\n",
    "        elif 5 <= score <= 7:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "\n",
    "    # Add the predictions to the dataframe by mapping the categorize_score function to the predictions\n",
    "    # prediction values will be low, medium, or high\n",
    "    df_train[f\"Predicted_{pred_type}_Risk_Group\"] = pd.Categorical(\n",
    "        pd.Series(y_pred).map(categorize_score),\n",
    "        categories=[\"Low\", \"Medium\", \"High\"],\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # groups based on predicted risk group and actual recidivism\n",
    "    # size() returns the number of rows in each group\n",
    "    # unstack() pivots the table so that the predicted risk group is the index and the two_year_recid is the column\n",
    "    predicted_grouped = df_train.groupby(\n",
    "        [f\"Predicted_{pred_type}_Risk_Group\", \"two_year_recid\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # save to csv\n",
    "    predicted_file_path = os.path.join(RESULTS_FOLDER, f\"predicted_vs_recid_{pred_type}.csv\")\n",
    "    predicted_grouped.to_csv(predicted_file_path)\n",
    "\n",
    "    # as above but add race to the grouping\n",
    "    race_comparison = df_train.groupby(\n",
    "        [\"race\", f\"Predicted_{pred_type}_Risk_Group\", \"two_year_recid\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # save to csv\n",
    "    race_comparison_file_path = os.path.join(RESULTS_FOLDER, f\"predicted_risk_by_race_{pred_type}_summary.csv\")\n",
    "    race_comparison.to_csv(race_comparison_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
