\section{Preparing the data for further analysis and training}

Before we can perform any analysis or apply machine learning techniques, it is important to pre-process and prepare the dataset so that we can handle missing values, encode categorical features, and split the data into training, testing, and validation sets. This step will produce a clean dataset for building accurate and unbiased models. The following steps outline the procedures to prepare the dataset for further analysis and training.

\subsection{Initial look at data and missing values handling}
After downloading the dataset for this exercise, the first step is to perform some preliminary analysis; namely, we look at the dataset structure, the first few records and some statistical information related to the numerical fields. The size of the dataset makes it very hard to visualise all the information on the screen. For this reason, we have developed a\textbf{\texttt{ReportWriter}}  class to save\textbf{\texttt{DataFrame}}  objects and images into an Excel file to analyse and manipulate data easily. We have used this class for all reporting requirements in this exercise instead of\textbf{\texttt{print}}  statements that are cumbersome and not persistent.

Some considerations after this initial analysis:

- There are a total of 7214 records in this dataset
- there is one field,\textbf{\texttt{violent\_recid}} , where all the fields are null.
-\textbf{\texttt{vr\_case\_number}} ,\textbf{\texttt{vr\_charge\_degree}} ,\textbf{\texttt{vr\_offense\_date}} ,\textbf{\texttt{vr\_charge\_desc}}  and\textbf{\texttt{c\_arrest\_date}}  have over 6000 missing entries.
-\textbf{\texttt{r\_days\_from\_arrest}} , \textbf{\texttt{r\_jail\_in}} and\textbf{\texttt{r\_jail\_out}}  have nearly 5000 null entries.
-\textbf{\texttt{r\_charge\_desc}} ,\textbf{\texttt{r\_case\_number}} ,\textbf{\texttt{r\_charge\_degree}}  and\textbf{\texttt{r\_offense\_date}}  have over 3000 missing records,\textbf{\texttt{c\_jail\_in}} ,\textbf{\texttt{c\_jail\_out}} ,\textbf{\texttt{in\_custody}} and\textbf{\texttt{out\_custody}}  have a few hundred missing entries}} .}} , 

After considering all the columns with a high proportion of missing data, we decided to remove all columns with less than 50% of fields that were not null. As a result of this step}} c\_arrest\_date}} ,\textbf{\texttt{r\_case\_number}} ,\textbf{\texttt{r\_charge\_degree}} ,\textbf{\texttt{r\_charge\_desc}} ,\textbf{\texttt{r\_days\_from\_arrest}} ,\textbf{\texttt{r\_jail\_in}} ,\textbf{\texttt{r\_jail\_out}} ,\textbf{\texttt{r\_offense\_date}} ,\textbf{\texttt{violent\_recid}} ,\textbf{\texttt{vr\_case\_number}} ,\textbf{\texttt{vr\_charge\_degree}} ,\textbf{\texttt{vr\_charge\_desc}}  and\textbf{\texttt{vr\_offense\_date}}  were removed from the dataset.

\subsection{Data cleaning, missing values and imputation}

The features\textbf{\texttt{compas\_screening\_date}} ,\textbf{\texttt{sex}} ,\textbf{\texttt{age}} ,\textbf{\texttt{age\_cat}} ,\textbf{\texttt{race}} ,\textbf{\texttt{juv\_misd\_count}} ,\textbf{\texttt{juv\_other\_count}} ,\textbf{\texttt{priors\_count}} ,\textbf{\texttt{days\_b\_screening\_arrest}} ,\textbf{\texttt{c\_charge\_degree}} ,\textbf{\texttt{two\_year\_recid}} ,\textbf{\texttt{decile\_score}} ,\textbf{\texttt{score\_text}}  are considered important to the dataset are then extracted so that they can be further analysed. 

While examining the resultant dataset, we noticed that\textbf{\texttt{days\_b\_screening\_arrest}}  has  6907 values that are not null. Whilst it is possible to eliminate the rows that contain the null values at this stage, we replaced the missing values using a KNN imputation technique by grouping the numeric values of this dataset so that we can calculate the missing values. We checked this process by plotting the distribution of\textbf{\texttt{days\_b\_screening \_arrest}}  before and after imputation to see if any variations occurred.